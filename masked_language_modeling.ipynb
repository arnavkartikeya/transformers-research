{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# End-to-end Masked Language Modeling with BERT\n",
    "\n",
    "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
    "**Date created:** 2020/09/18<br>\n",
    "**Last modified:** 2024/03/15<br>\n",
    "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "global_arr = [\"cat\"] \n",
    "class Dog:\n",
    "    def __init__(self, c):\n",
    "        global global_arr\n",
    "        global_arr.append(c)\n",
    "dog = Dog(\"dog\")\n",
    "print(global_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Masked Language Modeling is a fill-in-the-blank task,\n",
    "where a model uses the context words surrounding a mask token to try to predict what the\n",
    "masked word should be.\n",
    "\n",
    "For an input that contains one or more mask tokens,\n",
    "the model will generate the most likely substitution for each.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
    "- Output: \"I have watched this movie and it was awesome.\"\n",
    "\n",
    "Masked language modeling is a great way to train a language\n",
    "model in a self-supervised setting (without human-annotated labels).\n",
    "Such a model can then be fine-tuned to accomplish various supervised\n",
    "NLP tasks.\n",
    "\n",
    "This example teaches you how to build a BERT model from scratch,\n",
    "train it with the masked language modeling task,\n",
    "and then fine-tune this model on a sentiment classification task.\n",
    "\n",
    "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
    "to create a BERT Transformer-Encoder network architecture.\n",
    "\n",
    "Note: This example should be run with `tf-nightly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install `tf-nightly` via `pip install tf-nightly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 23:52:04.188222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 23:52:05.339035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17468383392548077030\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20615987200\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14035956237778422088\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:a1:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 23:52:08.260371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:a1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#checking the gpu is there (run this again after the next couple pip installs)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tf-nightly[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /opt/conda/lib/python3.11/site-packages (0.8.2)\n",
      "Requirement already satisfied: keras-core in /opt/conda/lib/python3.11/site-packages (0.1.7)\n",
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.11/site-packages (2.16.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-nlp keras-core tensorflow-text --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"all\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import WhitespaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Set-up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will first download the IMDB data and load into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  48.6M      0  0:00:01  0:00:01 --:--:-- 48.6M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = pd.concat([train_df, test_df])\n",
    "# train_df.concat(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Mary Poppins is definitely much better, but th...\n",
       "1    The show is at least partially Faked (So is no...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
    "It transforms a batch of strings into either\n",
    "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
    "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
    "\n",
    "Below, we define 3 preprocessing functions.\n",
    "\n",
    "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
    "2.  The `encode` function encodes raw text into integer token ids.\n",
    "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
    "It masks 15% of all input tokens in each sequence at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_for_debugging = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 23:52:20.412377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:a1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "instances = 0 \n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\\\"\"), \"\" # original line: \"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "\n",
    "#for decoding to create the masks \n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "\n",
    "def decode(tokens):\n",
    "    return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "def generate_mask(vectorized_txt): \n",
    "    global mask_for_debugging\n",
    "    # print('vectorized_txt:') \n",
    "    # print(len(vectorized_txt))\n",
    "    # print(vectorized_txt)\n",
    "    pos_vals = {\"D\", \"J\", \"N\", \"P\", \"V\", \"R\"}\n",
    "    map_to_pos = {\"D\":\"verb\", \"J\":\"adj\", \"N\":\"noun\", \"P\":\"pronoun\", \"V\":\"verb\", \"R\": \"adverb\"}\n",
    "    relationships = {\"noun\": {\"verb\", \"adj\", \"pronoun\"},  \"verb\":{\"noun\", \"adverb\", \"pronoun\"}, \"adj\":{\"noun\", \"pronoun\"}, \"pronoun\":{\"verb\", \"adj\", \"adverb\", \"noun\"}, \"adverb\": {\"verb\"}}\n",
    "\n",
    "    \n",
    "    decoded_txt = \"\" \n",
    "    word_count =  0 \n",
    "    #assuming the shape of vectorized_txt is (256,) \n",
    "    for i in range(len(vectorized_txt)):\n",
    "        vectorized_word = vectorized_txt[i] \n",
    "        if vectorized_word == 0:\n",
    "            break \n",
    "        else:\n",
    "            #doing this in case the last word is a specialized space token and it gets stripped at the end \n",
    "            if i != len(vectorized_txt)-1 and vectorized_txt[i+1] != 0 :\n",
    "                decoded_txt += decode([vectorized_word]) + \" \" \n",
    "            else:\n",
    "                decoded_txt += decode([vectorized_word]) \n",
    "            word_count += 1 \n",
    "    # print(word_count)\n",
    "    # print(decoded_txt)\n",
    "\n",
    "    remaining_vals = 256 - word_count #change this to config.MAX_SIZE afterwards \n",
    "    \n",
    "    # tokenized = word_tokenize(decoded_txt)\n",
    "    # tokenized=WhitespaceTokenizer().tokenize(decoded_txt)\n",
    "    tokenized = decoded_txt.split(\" \")\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    # print(f'length of tagged before processing {len(tagged)}') \n",
    "    # print(tagged)\n",
    "    word_pos = []\n",
    "    for j in range(len(tagged)): \n",
    "        word, pos = tagged[j] \n",
    "        if word == ']' and j != 0 and (tagged[j-1][0] == 'mask' or tagged[j-1][0] == 'UNK'):\n",
    "            word_pos.pop(-1)\n",
    "            word_pos.pop(-1)\n",
    "                # print(decoded_txt) \n",
    "                # sys.quit() \n",
    "            word_pos.append(None) \n",
    "        elif ord(word[0]) >= 127 and len(word) == 1 or pos[0] not in pos_vals: \n",
    "            word_pos.append(None)\n",
    "        else:\n",
    "            word_pos.append(map_to_pos[pos[0]]) \n",
    "\n",
    "    # print(f'after processing {len(word_pos)}')\n",
    "    # print(word_pos)\n",
    "\n",
    "    mask = np.ones((len(word_pos), len(word_pos)))\n",
    "\n",
    "    for row in range(mask.shape[0]):\n",
    "        row_word_pos = word_pos[row] \n",
    "        if row_word_pos: #the [CLS], [SEP], and [MASK] tokens will be allowed to be influenced by every word \n",
    "            allowed_pos = relationships[row_word_pos] \n",
    "            for col in range(mask.shape[1]): \n",
    "                #for now, don't worry about the [mask] token not being allowed to influence the words \n",
    "                if word_pos[col] not in allowed_pos: \n",
    "                    mask[row,col] = 0\n",
    "                # else:\n",
    "                #     mask[row, col] = 0 \n",
    "\n",
    "    np.fill_diagonal(mask, 1) \n",
    "\n",
    "    if remaining_vals > 0: \n",
    "        true_mask = np.zeros((256, 256)) #change this to config.MAX_SIZE afterwards \n",
    "        try:\n",
    "            true_mask[:word_count, :word_count] = mask \n",
    "        except:\n",
    "            print(f'mask shape: {mask.shape}') \n",
    "            print(f'word_count: {word_count}')\n",
    "            mask_for_debugging = vectorized_txt \n",
    "            print(decoded_txt)\n",
    "            sys.quit()\n",
    "        mask = true_mask\n",
    "    \n",
    "    return mask \n",
    "\n",
    "    \n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    all_syntactic_masks = [] \n",
    "    # c = 0 \n",
    "    for sentence in encoded_texts_masked:\n",
    "        # print(c)\n",
    "        all_syntactic_masks.append(generate_mask(sentence)) \n",
    "        # c += 1 \n",
    "\n",
    "    # count = 0\n",
    "    # for sentence in encoded_texts_masked:\n",
    "    #     if count == 3: \n",
    "    #         break \n",
    "    #     print(f'count: {count}, decoded sentence: {decode(sentence)}')\n",
    "    #     count += 1 \n",
    "\n",
    "    # #decode for masking \n",
    "    # all_syntactic_masks = [] \n",
    "    # for sentence in encoded_texts_masked: \n",
    "    #     decoded_sentence = decode(sentence) \n",
    "    #     temp = \"[CLS] \" + decoded_sentence + \" [SEP]\" \n",
    "    #     syntactic_mask = generate_mask(temp)\n",
    "    #     all_syntactic_masks.append(syntactic_mask) \n",
    "    # print(f'all_syntatic_masks shape: {len(all_syntactic_masks)}')\n",
    "    # # count = 1 \n",
    "    # # for sentence in encoded_texts_masked:\n",
    "    # #     if count == 3: \n",
    "    # #         break \n",
    "    # #     print(f'count: {count}, decoded sentence: {decode(sentence)}')\n",
    "    # #     count += 1 \n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights, all_syntactic_masks\n",
    "\n",
    "\n",
    "# We have 25000 examples for training\n",
    "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# We have 25000 examples for testing\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "# Prepare data for masked language model\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights, syntactic_masks = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   53,   576,    48, 29999,  1044,    17,     9, 29999,  1513,\n",
       "          69, 29999,    73,     5,     2,   724,   128, 29999,  3350,\n",
       "       13592, 29999,    72, 29999,  1168,    50,     2,  1065, 29999,\n",
       "          21, 29999,   491,    40,   636, 29999,     3,     2, 29999,\n",
       "         128,     8,  4294, 19231,    50,     2,   698,  1795,   529,\n",
       "           6,  1193,  6944,    43,    31,  1526,  7389,    38,    22,\n",
       "       29999,   576,   334,     2,  3230,    12,   264,     6,   308,\n",
       "          15, 29999,   265,    31,  7938,    92,   290,     3,    22,\n",
       "          57, 21862,   189, 29999,     6,     1,     2,   427, 20155,\n",
       "        1735,     2,    65, 29999,     2,     1, 18855,   197,    26,\n",
       "         145,   189,    18,     2,    17,    97, 29999,   222,   200,\n",
       "          31, 29999,   303,  5411, 27337,  2301,  3672,     6, 29999,\n",
       "          47,    66, 17058, 17334,     8,     2,   437,   295,     2,\n",
       "           1,   128, 28152,  4294, 19231,    56,  3872,   337,     2,\n",
       "         422, 18855,   187,     7, 29999,     8,     4,   812,     5,\n",
       "       29999,   196,  5451,  1526,   102, 29999,    23,     1,     1,\n",
       "          10,   155, 10622,     3,  1564,   771,     1,    30,    31,\n",
       "           2, 19482,   108, 29999,    44,   457,    11,     1, 29999,\n",
       "           2,    17,    12,     7,     2,  1004,   133,    66,    70,\n",
       "          93,   435,   408,    16,    12,    18,    43,     2,   225,\n",
       "           7, 14571, 29999,  4216,    54,     8, 14142, 21140, 22170,\n",
       "           9,     7, 29999, 14571,     2,   769,   591,     1,    81,\n",
       "        1073, 29999,    76,     1,  2405, 29999,  5694, 12912, 20164,\n",
       "       29999,   594,  1261,    86,   156,    27,  4367,    99,    51,\n",
       "       29999,  1160,     2, 29999,  2616,     1,     7,    33,  2060,\n",
       "          98,   576,     2,   848,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_for_debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = np.ones((256,1))\n",
    "inp_mask = np.random.rand(*encoded_texts.shape) < 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   10,   228,   447,  5965,     6,    11,   118,    10,   173,\n",
       "         846,  2285,     2,  1241,   264,    32,     6,    64,     2,\n",
       "         356,   739,    29,     4,    80,    65,   361,     3,     9,\n",
       "          45,     2,   115,   149,     3,  1612,    20,     2,   118,\n",
       "          10,    77,  3018,     8,   167,  1241,     6,   103,     9,\n",
       "          54,    43,    10,   228,    21,   338,    10,   205,    25,\n",
       "          53,  7401,   284,     6,  1899, 16500, 29222,  2580,  5054,\n",
       "           7,     2,   115,   294,    20,     2,   118,     9,     7,\n",
       "        2428,     3,  1125,    10,   102,    11,   118,   140,   764,\n",
       "          20,     2,   940,     3,    10,   252,   298,   140,  3018,\n",
       "           8,     6,   103,     9,    10,   204,     2,    52,    88,\n",
       "         389,     3,   155,    10,   277,   162,     6,   103,     9,\n",
       "          18,    10,    13,   144,  2584,    28,   260,     3,    10,\n",
       "         849,     6,   103,     9,    84,     9,    13,    20,     3,\n",
       "          10,   409,   110,     9,     3,   201,   147,    29,    53,\n",
       "         506,   118,    10,   228,    62,   382,     9,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all_review[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 142)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_masks[50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_all_review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_all_review\u001b[49m[\u001b[38;5;241m11149\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_all_review' is not defined"
     ]
    }
   ],
   "source": [
    "x_all_review[11149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[mask]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = '[CLS] loony tunes [mask] ventured at least twice [mask] the future the first time was with the brilliantly [mask] duck [UNK] the latter time was with this ",
    " um ",
    " effort [UNK] unleashed [mask] without merit and might be considered [mask] good product were it not that it isnt up [mask] warner brothers quality wb [mask] are noted for their cheeky humor appealing at [mask] as much to [mask] as to children these pedestrian superhero episodes on the other hand cannot fail [mask] convince adults to pass them up the premise of the series [mask] that 6 ordinary individuals 2 bunnies [mask] [UNK] [mask] a [mask] a [mask] and a coyote live on the [UNK] of [UNK] and acquire super powers when a meteor strikes the planet in [UNK] whats confusing is that the titles section features these individuals with a [UNK] to [UNK] from the 21st century cute but [UNK] stupid in each episode the [mask] [UNK]  amid [mask] amusing but essentially banal banter  fight various super [mask] [mask] the most part these are types that appear in [mask] mediocre [mask] adventure series and even some of the better ones like many [mask] superhero series this one takes [mask] villains far [mask] seriously [mask] the context and of course these guys are the only [mask] that laugh  the usual evil laugh of course [mask] is it that villains [mask] predictable [mask] adventures always  always  [mask] [UNK] at every opportunity animated material of [mask] sort seems [mask] leave laughter exclusively in the [SEP]'\n",
    "decode(vectorize_layer(['[MASK]'])[0].numpy())\n",
    "# vectorize_layer([txt])[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.review.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As predictable as a Hallmark card, but not without merit, The Rookie makes for a solid outing. Dennis Quaid, the most reasonable jock actor working today, is absolutely perfect as the science teacher turned baseball player Jimmy Morris. The film is never dumbed down for the children, as would be expected from a G rated film. As a sports film, The Rookie is one of the best I have seen since Any Given Sunday.',\n",
       " \"If The Man in the White Suit had been done in America, can't you see either Danny Kaye or Jerry Lewis trying on Alec Guinness's Sidney Stratton on for size?<br /><br />This is one of the best of Alec Guinness's films and certainly one of the best that Ealing Studios in the United Kingdom ever turned out. It's so perfectly fits within the time frame of the new Labour government and the society it was trying to build. It's amazing how in times of crisis capital and labor can agree.<br /><br />Alec Guinness this meek little schnook of a man is obsessed with the idea that he can invent clothing that will never need cleaning, that in fact repels all kinds of foreign matter the minute it touches the garment. <br /><br />He's a persistent cuss and he does succeed. Of course the implications haven't really been thought through about the kind of impact clothing like that will have on society. In the end everyone is chasing him down like they would a fugitive, almost like Peter Lorre from M or Orson Welles in The Stranger or even Robert Newton in Oliver Twist. <br /><br />It's the mark of a great comedy film that a potentially serious situation like that chase as described in some of the serious films I've mentioned can be played for laughs. Poor Guinness's suit is not only white and stain repellent, but it glows like a neon sign.<br /><br />Other than Guinness the best performances are from Cecil Parker as yet another pompous oaf, Joan Greenwood as his siren daughter and Ernest Thesiger the biggest clothing manufacturer in the UK> <br /><br />Come to think of it, did Paramount borrow that suit from Ealing and give it to John Travolta for Saturday Night Fever?\",\n",
       " \"I went to see this movie at our college theater thirty years ago because I liked Bruce Dern in Silent Running and Family Plot. To this day (sorry Jack Nicholson), it is still the dullest movie I've ever seen. It just went on and on with no discernible point and then - it just ended. The lights came up and I watched everyone looking around in confusion. Had the projectionist missed a reel? I've never had the urge to find out. All I remember about the movie is that it was a non-drama about some annoying college basketball players and their coach. The most enjoyable part of the movie was watching the totally mystified audience afterwords. Fortunately, this was just an exception for Jack, Bruce, and Karen Black.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.review.values.tolist()[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create BERT model (Pretraining Model) for masked language modeling\n",
    "\n",
    "We will create a BERT-like pretraining model architecture\n",
    "using the `MultiHeadAttention` layer.\n",
    "It will take token ids as inputs (including masked tokens)\n",
    "and it will predict the correct ids for the masked input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "attentions = [] \n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        global attentions \n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "        # print(f'attention matrix shape {weights.shape}')\n",
    "        if len(attentions) == 1:\n",
    "            attentions.pop() \n",
    "        # print(\"appending to attentions\")\n",
    "        attentions.append(weights)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, output_attention=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1) (32, 8, 256, 128?) \n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # print(f'attention shape: {o_reshaped.shape}')\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\"linear warm up - linear decay\"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / self.decay_epochs,\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        epoch = tf.cast(epoch, \"float32\")\n",
    "        return self.calculate_lr(epoch)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=CustomSchedule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)  (None, 256, 128)             3840000   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " position_embedding (Positi  (None, 256, 128)             32768     ['word_embedding[0][0]']      \n",
      " onEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 256, 128)             0         ['word_embedding[0][0]',      \n",
      " Lambda)                                                             'position_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0_multiheadattenti  (None, 256, 128)             8368      ['tf.__operators__.add[0][0]',\n",
      " on (MultiHeadAttention)                                             'tf.__operators__.add[0][0]',\n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " encoder_0_att_dropout (Dro  (None, 256, 128)             0         ['encoder_0_multiheadattention\n",
      " pout)                                                              [0][0]']                      \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 256, 128)             0         ['tf.__operators__.add[0][0]',\n",
      " OpLambda)                                                           'encoder_0_att_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0_att_layernormali  (None, 256, 128)             256       ['tf.__operators__.add_1[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " encoder_0_ffn (Sequential)  (None, 256, 128)             33024     ['encoder_0_att_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      " encoder_0_ffn_dropout (Dro  (None, 256, 128)             0         ['encoder_0_ffn[0][0]']       \n",
      " pout)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 256, 128)             0         ['encoder_0_att_layernormaliza\n",
      " OpLambda)                                                          tion[0][0]',                  \n",
      "                                                                     'encoder_0_ffn_dropout[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_0_ffn_layernormali  (None, 256, 128)             256       ['tf.__operators__.add_2[0][0]\n",
      " zation (LayerNormalization                                         ']                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)             (None, 256, 30000)           3870000   ['encoder_0_ffn_layernormaliza\n",
      "                                                                    tion[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7784672 (29.70 MB)\n",
      "Trainable params: 7784672 (29.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bert_module(query, key, value, i, use_custom=True):\n",
    "    # Multi headed self-attention\n",
    "    attention_ouput=None\n",
    "    if use_custom:\n",
    "        attention_output = MultiHeadAttention(h=config.NUM_HEAD, d_k=(config.EMBED_DIM // config.NUM_HEAD), d_v=(config.EMBED_DIM // config.NUM_HEAD), d_model=config.EMBED_DIM, name=\"encoder_{}_multiheadattention\".format(i))(query, key, value)\n",
    "    else:\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=config.NUM_HEAD,\n",
    "            key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "            name=\"encoder_{}_multiheadattention\".format(i),\n",
    "        )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}_att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}_att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}_ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}_ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}_ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=\"none\")\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        #here is where we would seperate the input and the masks \n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = keras_nlp.layers.PositionEmbedding(\n",
    "        sequence_length=config.MAX_LEN\n",
    "    )(word_embeddings)\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    # optimizer = keras.optimizers.AdamW(learning_rate=config.LR)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=CustomSchedule())\n",
    "    mlm_model.compile(optimizer=optimizer, metrics=[\"accuracy\"], run_eagerly=True) #add parameter run_eagerly=True for running in eager mode\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),#add sample_tokens[0].numpy() if it doesnt work\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "\n",
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 03:23:21.462766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-04-01 03:23:22.010372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5600a713afb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-01 03:23:22.010447: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-04-01 03:23:22.032254: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-01 03:23:22.195035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-04-01 03:23:22.380322: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f46ca6fee80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f46ca6fee80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 26ms/steps - loss: 7.75\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.05356184}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.035384145}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.034424987}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.033160124}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.029096832}\n",
      "1563/1563 [==============================] - 184s 116ms/step - loss: 7.7546\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 0s 49ms/steps - loss: 6.66\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.05719444}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.053628176}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.043127198}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.040847708}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.03225567}\n",
      "1563/1563 [==============================] - 124s 79ms/step - loss: 6.6685\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 0s 30ms/steps - loss: 6.20\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.06336822}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.060066216}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.039594755}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.035114087}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.034150723}\n",
      "1563/1563 [==============================] - 118s 75ms/step - loss: 6.2052\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 0s 30ms/steps - loss: 5.84\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09587751}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.07185799}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.04897171}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'i have watched this a and it was awesome',\n",
      " 'probability': 0.029058347}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'of',\n",
      " 'prediction': 'i have watched this of and it was awesome',\n",
      " 'probability': 0.027079921}\n",
      "1563/1563 [==============================] - 115s 74ms/step - loss: 5.8402\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 0s 28ms/steps - loss: 5.60\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09736628}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.078052744}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.06846138}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02964648}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.02502888}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 5.6039\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 0s 32ms/steps - loss: 5.45\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.15548165}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.06963264}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.05204254}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028322063}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.02045301}\n",
      "1563/1563 [==============================] - 112s 72ms/step - loss: 5.4565\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 0s 38ms/steps - loss: 5.34\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.12831658}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.07321473}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.0705656}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.030053025}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.018786069}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 5.3479\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 0s 32ms/steps - loss: 5.25\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.13296208}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.0833807}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08179113}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02775211}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.019865314}\n",
      "1563/1563 [==============================] - 113s 72ms/step - loss: 5.2590\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 0s 30ms/steps - loss: 5.18\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.1282676}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.076308474}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.072626516}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.030302782}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.020111812}\n",
      "1563/1563 [==============================] - 112s 72ms/step - loss: 5.1824\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 0s 26ms/steps - loss: 5.11\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.13231187}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.09168184}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.07878777}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.030202456}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'was',\n",
      " 'prediction': 'i have watched this was and it was awesome',\n",
      " 'probability': 0.022480471}\n",
      "1563/1563 [==============================] - 112s 72ms/step - loss: 5.1147\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 0s 26ms/steps - loss: 5.05\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.111223355}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.09173727}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.06750343}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.029931765}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'my',\n",
      " 'prediction': 'i have watched this my and it was awesome',\n",
      " 'probability': 0.021322599}\n",
      "1563/1563 [==============================] - 113s 72ms/step - loss: 5.0559\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 5.00\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.10787541}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08175504}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.07098092}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.030525077}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'my',\n",
      " 'prediction': 'i have watched this my and it was awesome',\n",
      " 'probability': 0.023448538}\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 5.0070\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 0s 28ms/steps - loss: 4.97\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.09405918}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09181363}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08386142}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028866014}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.023798876}\n",
      "1563/1563 [==============================] - 115s 74ms/step - loss: 4.9763\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 0s 26ms/steps - loss: 4.95\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09345891}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.09115196}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.085166514}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028294517}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024026029}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9561\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 4.95\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09238179}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.08983528}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.0847456}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02841458}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.023944527}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9514\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 4.94\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.093384355}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.08997762}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08369594}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028197654}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.023987798}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9485\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 4.94\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.092653506}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.0916769}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.084472105}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028268367}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024054192}\n",
      "1563/1563 [==============================] - 115s 73ms/step - loss: 4.9460\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 4.94\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09234373}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.09018085}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08377418}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028385865}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024035972}\n",
      "1563/1563 [==============================] - 115s 73ms/step - loss: 4.9436\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 0s 25ms/steps - loss: 4.94\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09127313}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.089351155}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08404987}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02850311}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.023900358}\n",
      "1563/1563 [==============================] - 115s 74ms/step - loss: 4.9424\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 0s 35ms/steps - loss: 4.94\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.092200525}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.08959079}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.0832979}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02816015}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.023998674}\n",
      "1563/1563 [==============================] - 113s 72ms/step - loss: 4.9404\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 0s 30ms/steps - loss: 4.93\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09096025}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.08973744}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08353679}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028288862}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024189156}\n",
      "1563/1563 [==============================] - 111s 71ms/step - loss: 4.9385\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 0s 27ms/steps - loss: 4.93\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09105682}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.089817286}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.083224654}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.02843464}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024046268}\n",
      "1563/1563 [==============================] - 113s 72ms/step - loss: 4.9370\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 0s 40ms/steps - loss: 4.93\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09123127}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.08965272}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.082736775}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028388957}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024230039}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9348\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 0s 37ms/steps - loss: 4.93\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.09038645}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.089862764}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.08292452}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028241428}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024249054}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9339\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 0s 32ms/steps - loss: 4.93\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'movie',\n",
      " 'prediction': 'i have watched this movie and it was awesome',\n",
      " 'probability': 0.090941414}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'this',\n",
      " 'prediction': 'i have watched this this and it was awesome',\n",
      " 'probability': 0.09031394}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'i have watched this i and it was awesome',\n",
      " 'probability': 0.082668416}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'it',\n",
      " 'prediction': 'i have watched this it and it was awesome',\n",
      " 'probability': 0.028179377}\n",
      "{'input_text': 'i have watched this [mask] and it was awesome',\n",
      " 'predicted mask token': 'to',\n",
      " 'prediction': 'i have watched this to and it was awesome',\n",
      " 'probability': 0.024213964}\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 4.9317\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bert_masked_model\u001b[38;5;241m.\u001b[39mfit(mlm_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[generator_callback])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbert_masked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert_mlm_imdb.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/optimizers/schedules/learning_rate_schedule.py:83\u001b[0m, in \u001b[0;36mLearningRateSchedule.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@abc\u001b[39m\u001b[38;5;241m.\u001b[39mabstractmethod\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate schedule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust override `get_config()` in order to be serializable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable."
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=25, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"bert_mlm_imdb.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[6.2163699e-02 1.7237583e-02 8.2696289e-02 ... 1.0992441e-03\n",
      "    7.6986058e-04 1.5021606e-03]\n",
      "   [1.2154877e-02 7.5936848e-03 1.3497258e-02 ... 2.7682991e-03\n",
      "    2.4291540e-03 3.1041517e-03]\n",
      "   [1.0017248e-01 1.9970922e-02 1.4273998e-01 ... 6.1508315e-04\n",
      "    3.9450894e-04 9.1180980e-04]\n",
      "   ...\n",
      "   [3.1742404e-04 8.2923233e-04 2.5627049e-04 ... 6.5006632e-03\n",
      "    8.4893536e-03 5.1458795e-03]\n",
      "   [1.2958751e-04 4.7003632e-04 9.7245524e-05 ... 7.4491552e-03\n",
      "    1.0656916e-02 5.4439828e-03]\n",
      "   [1.8146719e-04 5.7994563e-04 1.4034878e-04 ... 7.0642545e-03\n",
      "    9.7418763e-03 5.3218575e-03]]\n",
      "\n",
      "  [[1.7115727e-02 1.9844633e-02 2.5542544e-02 ... 2.0763951e-03\n",
      "    1.4171037e-03 2.2735444e-03]\n",
      "   [8.5389279e-03 9.7096181e-03 1.1389668e-02 ... 2.8663762e-03\n",
      "    2.2410376e-03 2.9508844e-03]\n",
      "   [3.1036781e-02 3.2134797e-02 4.1619256e-02 ... 1.5916503e-03\n",
      "    1.0926903e-03 1.9199133e-03]\n",
      "   ...\n",
      "   [2.0031934e-03 1.8061243e-03 1.5893610e-03 ... 4.7820858e-03\n",
      "    5.8247927e-03 4.6744379e-03]\n",
      "   [1.8170547e-03 1.5799976e-03 1.3485452e-03 ... 4.9314615e-03\n",
      "    6.3060652e-03 4.8391162e-03]\n",
      "   [2.2348652e-03 1.9535513e-03 1.7072457e-03 ... 4.6910117e-03\n",
      "    5.7943673e-03 4.6748109e-03]]\n",
      "\n",
      "  [[3.8666658e-02 2.4151979e-02 5.4583594e-02 ... 1.4500396e-03\n",
      "    1.1820052e-03 1.5120603e-03]\n",
      "   [2.2480749e-02 1.6570989e-02 2.8779421e-02 ... 2.1019042e-03\n",
      "    1.8335059e-03 2.1453931e-03]\n",
      "   [1.2900361e-02 1.1856799e-02 1.5040180e-02 ... 2.7950604e-03\n",
      "    2.6513149e-03 2.7440365e-03]\n",
      "   ...\n",
      "   [3.2172601e-03 3.5407387e-03 3.1207583e-03 ... 4.1753328e-03\n",
      "    4.3223328e-03 4.0936209e-03]\n",
      "   [1.8388516e-03 2.1592190e-03 1.6633273e-03 ... 4.7197910e-03\n",
      "    5.0452962e-03 4.6300539e-03]\n",
      "   [2.3633698e-03 2.6686024e-03 2.2072142e-03 ... 4.4631753e-03\n",
      "    4.6878522e-03 4.3895058e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0827929e-01 2.4088956e-02 8.8348046e-02 ... 6.3732557e-04\n",
      "    4.9235130e-04 6.9494534e-04]\n",
      "   [5.6102324e-02 1.8908259e-02 4.1132689e-02 ... 1.7243152e-03\n",
      "    1.4560608e-03 1.7968962e-03]\n",
      "   [2.4244560e-01 2.0727888e-02 1.3223395e-01 ... 8.0409278e-05\n",
      "    5.4293763e-05 8.9380839e-05]\n",
      "   ...\n",
      "   [1.5764874e-05 1.0524886e-04 2.8497732e-05 ... 6.4052101e-03\n",
      "    8.5579176e-03 5.9997919e-03]\n",
      "   [6.0429975e-06 5.4910182e-05 1.1589484e-05 ... 6.8653575e-03\n",
      "    9.6531017e-03 6.3321260e-03]\n",
      "   [1.1269375e-05 8.3933985e-05 2.0905109e-05 ... 6.5493700e-03\n",
      "    8.9058029e-03 6.1051468e-03]]\n",
      "\n",
      "  [[4.8184919e-01 5.4640070e-02 3.1306672e-01 ... 1.0814895e-05\n",
      "    1.1051542e-05 2.6619678e-05]\n",
      "   [5.4193956e-01 4.3886639e-02 3.1199083e-01 ... 2.9919540e-06\n",
      "    3.1388856e-06 8.1384205e-06]\n",
      "   [5.8500046e-01 3.4219008e-02 3.1053767e-01 ... 7.0514938e-07\n",
      "    7.4743025e-07 2.1680128e-06]\n",
      "   ...\n",
      "   [1.5259227e-04 3.4739735e-04 1.9399663e-04 ... 6.0861781e-03\n",
      "    5.8429739e-03 4.5792549e-03]\n",
      "   [5.9753296e-05 1.6625086e-04 7.7495679e-05 ... 7.0084734e-03\n",
      "    6.7714746e-03 4.7840066e-03]\n",
      "   [5.3339772e-05 1.5364513e-04 7.0434828e-05 ... 7.0444252e-03\n",
      "    6.7719026e-03 4.7784611e-03]]\n",
      "\n",
      "  [[6.6942968e-03 6.2964284e-03 1.0878742e-02 ... 2.8941317e-03\n",
      "    2.5691534e-03 2.9273559e-03]\n",
      "   [5.4240539e-03 5.4195952e-03 7.2732624e-03 ... 3.2695376e-03\n",
      "    3.0271704e-03 3.2704067e-03]\n",
      "   [8.1202853e-03 8.0026640e-03 1.6062366e-02 ... 2.5023273e-03\n",
      "    2.0957424e-03 2.5096957e-03]\n",
      "   ...\n",
      "   [3.1673214e-03 3.1650332e-03 2.6443307e-03 ... 4.3243794e-03\n",
      "    4.5349821e-03 4.3246457e-03]\n",
      "   [2.5447267e-03 2.5324479e-03 1.7671483e-03 ... 4.7739018e-03\n",
      "    5.2578426e-03 4.7774492e-03]\n",
      "   [2.8599063e-03 2.8991445e-03 2.1928512e-03 ... 4.5231921e-03\n",
      "    4.8411195e-03 4.5114453e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "attention = attentions[0]\n",
    "attention = attention.numpy()\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = vectorize_layer([\"I didn't watch this [mask] and it was awesome\"]).numpy()\n",
    "prediction = bert_masked_model.predict(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "attention = attentions[0].numpy()\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 11, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(attention[0][:, :11, :11], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = np.expand_dims(attention[0][:, :11, :11], axis=0)\n",
    "fin = torch.from_numpy(test)\n",
    "fin = (fin,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin[0][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(fin[0].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertviz\n",
      "  Obtaining dependency information for bertviz from https://files.pythonhosted.org/packages/66/07/cce3d29605a3011d3685b2041fb94fcad25565b80bd2f22f3dcd75b2eee9/bertviz-1.4.0-py3-none-any.whl.metadata\n",
      "  Downloading bertviz-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers>=2.0 (from bertviz)\n",
      "  Obtaining dependency information for transformers>=2.0 from https://files.pythonhosted.org/packages/e2/52/02271ef16713abea41bab736dfc2dbee75e5e3512cf7441e233976211ba5/transformers-4.39.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.11/site-packages (from bertviz) (2.0.0+cu117)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from bertviz) (4.66.1)\n",
      "Collecting boto3 (from bertviz)\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/fe/61/2561a979dabf221724b0de8d5ba9c6f42950fea689ebfca304e8ee943d68/boto3-1.34.74-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.74-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bertviz) (2.31.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from bertviz) (2023.10.3)\n",
      "Collecting sentencepiece (from bertviz)\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/fb/12/2f5c8d4764b00033cf1c935b702d3bb878d10be9f0b87f0253495832d85f/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0->bertviz) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.0->bertviz) (3.27.6)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.0->bertviz) (17.0.2)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers>=2.0->bertviz)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/05/c0/779afbad8e75565c09ffa24a88b5dd7e293c92b74eb09df6435fc58ac986/huggingface_hub-0.22.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=2.0->bertviz)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/15/0b/c09b2c0dc688c82adadaa0d5080983de3ce920f4a5cbadb7eaa5302ad251/tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=2.0->bertviz)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/8e/cf/b32d236cc01429bf2827bd1d8d81fa5a34a6cc7fb3499506f5d1aa19dcb8/safetensors-0.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.74 (from boto3->bertviz)\n",
      "  Obtaining dependency information for botocore<1.35.0,>=1.34.74 from https://files.pythonhosted.org/packages/08/03/33b2a745333c676c3ecd9627146c84ec600ad46794c352807f1ad0f5f3e5/botocore-1.34.74-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.34.74-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->bertviz)\n",
      "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/83/37/395cdb6ee92925fa211e55d8f07b9f93cf93f60d7d4ce5e66fd73f1ea986/s3transfer-0.10.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bertviz) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bertviz) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->bertviz) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.74->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=2.0->bertviz) (2023.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.74->boto3->bertviz) (1.16.0)\n",
      "Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.34.74-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.34.74-py3-none-any.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m148.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m194.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, safetensors, jmespath, huggingface-hub, botocore, tokenizers, s3transfer, transformers, boto3, bertviz\n",
      "Successfully installed bertviz-1.4.0 boto3-1.34.74 botocore-1.34.74 huggingface-hub-0.22.2 jmespath-1.0.1 s3transfer-0.10.1 safetensors-0.4.2 sentencepiece-0.2.0 tokenizers-0.15.2 transformers-4.39.2\n"
     ]
    }
   ],
   "source": [
    "! pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "didn't\n"
     ]
    }
   ],
   "source": [
    "print('didn\\'t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['[CLS]', 'I', 'didn\\'t',  'watch', 'this', '[mask]', 'and', 'it' , 'was' ,'awesome', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attention has 12 positions, while number of tokens is 11 for tokens: [CLS] I didn't watch this [mask] and it was awesome [SEP]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_view\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/bertviz/model_view.py:202\u001b[0m, in \u001b[0;36mmodel_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, display_mode, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers, include_heads, html_action)\u001b[0m\n\u001b[1;32m    200\u001b[0m attn_seq_len_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_seq_len_left \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_text\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_seq_len_left\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positions, while number of tokens is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m attn_seq_len_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_seq_len_right \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright_text\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[0;31mValueError\u001b[0m: Attention has 12 positions, while number of tokens is 11 for tokens: [CLS] I didn't watch this [mask] and it was awesome [SEP]"
     ]
    }
   ],
   "source": [
    "from bertviz import model_view\n",
    "model_view(fin, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (attention)\n",
    "type(test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0][:15, :15].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(attention[0][0][:15, :15], cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 256)\n"
     ]
    }
   ],
   "source": [
    "maxes = np.max(attention[0], axis=2)\n",
    "print(maxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00404038 0.00431366 0.00800652 0.00412699 0.06825823 0.00599015\n",
      " 0.0046205  0.0042967  0.01767619 0.01275809 0.01308699 0.01029907\n",
      " 0.01391248 0.01109591 0.00975572 0.0130958  0.01381197 0.01604069\n",
      " 0.01219762 0.01285662 0.01153841 0.00933414 0.01637969 0.01421819\n",
      " 0.01336755 0.0125869  0.00819856 0.0137931  0.01253239 0.01143891\n",
      " 0.01325539 0.01223965 0.0111608  0.01391155 0.01352924 0.01043962\n",
      " 0.01208384 0.00876712 0.01019692 0.00983819 0.00643459 0.01089945\n",
      " 0.01103907 0.00815692 0.00821381 0.00880244 0.01061832 0.00927905\n",
      " 0.00969109 0.00941637 0.01083732 0.00668062 0.0070617  0.01109327\n",
      " 0.01069188 0.01025999 0.00866607 0.00784996 0.00874485 0.0124323\n",
      " 0.0110821  0.00773306 0.00919802 0.01015149 0.01082272 0.00836784\n",
      " 0.00893945 0.00931748 0.01066061 0.00738861 0.01168074 0.01132177\n",
      " 0.00689708 0.00781994 0.00681903 0.01027065 0.01008957 0.01088495\n",
      " 0.00759991 0.01004521 0.00975106 0.00587125 0.00814002 0.00858609\n",
      " 0.00871364 0.0117429  0.01105649 0.0088146  0.00699021 0.00752515\n",
      " 0.00678578 0.00659344 0.00696387 0.00764327 0.00659854 0.01040182\n",
      " 0.00650174 0.0063303  0.00903456 0.00770386 0.00444294 0.00799065\n",
      " 0.00875429 0.01286496 0.00612868 0.0084235  0.00818134 0.00643333\n",
      " 0.0080219  0.01018023 0.00446062 0.0083214  0.00626429 0.0083213\n",
      " 0.00460976 0.00579774 0.00658396 0.00670729 0.00462414 0.0070845\n",
      " 0.00480023 0.00539333 0.00659572 0.00586738 0.00580503 0.00665495\n",
      " 0.00466763 0.00535391 0.00469168 0.00560232 0.00714412 0.00505965\n",
      " 0.00561001 0.00486306 0.00604342 0.00446079 0.00504033 0.0061814\n",
      " 0.00547023 0.0060449  0.00468322 0.00542524 0.0045777  0.00527992\n",
      " 0.00444885 0.0048275  0.00442315 0.00465509 0.00433644 0.00519011\n",
      " 0.00435687 0.00507688 0.00445741 0.00482802 0.00499325 0.00457303\n",
      " 0.00454211 0.00468987 0.00461343 0.00692948 0.0063295  0.00452077\n",
      " 0.0062146  0.00532603 0.00458461 0.00482855 0.00440538 0.00481012\n",
      " 0.0056044  0.00450983 0.00463079 0.00453063 0.0046115  0.00455765\n",
      " 0.00436629 0.00456158 0.00459644 0.00443059 0.00583204 0.00558525\n",
      " 0.00459979 0.00493758 0.00544773 0.00492442 0.00644207 0.00450874\n",
      " 0.00461974 0.00483928 0.0049369  0.00442583 0.0045268  0.00579424\n",
      " 0.00610423 0.00440655 0.00448505 0.00452681 0.00454397 0.0043426\n",
      " 0.00469343 0.00517435 0.00456491 0.00445836 0.00462339 0.00466857\n",
      " 0.00452517 0.00483702 0.00502397 0.00430714 0.00472228 0.00448326\n",
      " 0.00453332 0.00470175 0.00478408 0.00484373 0.00490564 0.00458758\n",
      " 0.00464279 0.00463283 0.00455168 0.00443244 0.00455833 0.00450907\n",
      " 0.00444787 0.00558414 0.00471969 0.00451312 0.0047082  0.00480009\n",
      " 0.00467556 0.00444171 0.00457218 0.00509983 0.0055971  0.0045736\n",
      " 0.00562878 0.00451217 0.00463327 0.00465371 0.00457296 0.0046483\n",
      " 0.00445402 0.00458804 0.00544553 0.00452018 0.00454308 0.00445149\n",
      " 0.00469564 0.00455447 0.0045344  0.00460228 0.00459529 0.00476429\n",
      " 0.00492054 0.00440542 0.00447996 0.00475509]\n"
     ]
    }
   ],
   "source": [
    "print(maxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "k = 5\n",
    "\n",
    "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"]).numpy()\n",
    "prediction = bert_masked_model.predict(sample_tokens)\n",
    "masked_index = np.where(sample_tokens == mask_token_id)[1]\n",
    "mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "top_indices = mask_prediction[0].argsort()[-1*k:][::-1]\n",
    "values = mask_prediction[0][top_indices]\n",
    "\n",
    "def decode(tokens):\n",
    "    return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "def convert_ids_to_tokens(id):\n",
    "    return id2token[id]\n",
    "\n",
    "for i in range(len(top_indices)):\n",
    "    p = top_indices[i]\n",
    "    v = values[i]\n",
    "    tokens = np.copy(sample_tokens[0])\n",
    "    tokens[masked_index[0]] = p\n",
    "    result = {\n",
    "        \"input_text\": decode(sample_tokens[0]),\n",
    "        \"prediction\": decode(tokens),\n",
    "        \"probability\": v,\n",
    "        \"predicted mask token\": convert_ids_to_tokens(p),\n",
    "    }\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "mlm_model = keras.models.load_model(\n",
    "    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    ")\n",
    "pretrained_bert_model = keras.Model(\n",
    "    mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output\n",
    ")\n",
    "\n",
    "# Freeze it\n",
    "pretrained_bert_model.trainable = False\n",
    "\n",
    "\n",
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.AdamW(lr=1e-5)\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "\n",
    "classifer_model = create_classifier_bert_model()\n",
    "classifer_model.summary()\n",
    "\n",
    "# Train the classifier with frozen BERT stage\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")\n",
    "\n",
    "# Unfreeze the BERT model for fine-tuning\n",
    "pretrained_bert_model.trainable = True\n",
    "optimizer = keras.optimizers.Adam()\n",
    "classifer_model.compile(\n",
    "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create an end-to-end model and evaluate it\n",
    "\n",
    "When you want to deploy a model, it's best if it already includes its preprocessing\n",
    "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
    "production environment. Let's create an end-to-end model that incorporates\n",
    "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
    "as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_end_to_end(model):\n",
    "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a way to visualize the attention\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting (mlm task)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mlm_and_finetune_with_bert",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
