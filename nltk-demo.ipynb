{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236f8ba1-a6f0-4ea8-b92d-c0e533e45f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e00c58a-1fa7-4acc-9209-53be43b7148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 00:14:31.697392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 00:14:32.228291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56936e-92be-4b9d-af81-7bfa477c04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download(\"all\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bff76c-bd57-4810-b404-495dfee58ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62bfaaf0-3f45-416c-aa02-38d723115e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'JJ'), ('CLS', 'NNP'), (']', 'NNP'), ('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('A-list', 'NNP'), ('\\x86', 'NNP'), ('hit', 'VBD'), ('his', 'PRP$'), ('[', 'JJ'), ('mask', 'NN'), (']', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('store', 'NN'), ('[', 'NNP'), ('SEP', 'NNP'), (']', 'NN')]\n",
      "[('[', 'JJ'), ('CLS', 'NNP'), (']', 'NNP'), ('I', 'PRP'), (\"didn't\", 'VBD'), ('A-list', 'NNP'), ('\\x86', 'NNP'), ('hit', 'VBD'), ('his', 'PRP$'), ('[', 'JJ'), ('mask', 'NN'), (']', 'NN'), ('I', 'PRP'), ('was', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('store', 'NN'), ('[', 'NNP'), ('SEP', 'NNP'), (']', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "txt = \"[CLS] I didn't A-list \\x86 hit his [mask] I was at the store [SEP]\" \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# txt = txt.replace(\"[CLS]\", \"CLS\") \n",
    "# txt = txt.replace(\"]\", \"\") \n",
    "# txt = txt.replace(\"[mask]\", \"MASK\") \n",
    "\n",
    "tokenized = sent_tokenize(txt)\n",
    "\n",
    "\n",
    "for i in tokenized: \n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    \n",
    "    # removing stop words from wordList\n",
    "    # wordsList = [w for w in wordsList] \n",
    "    \n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    \n",
    "    print(tagged)\n",
    "    \n",
    "#in cases of apostrophes, merge the word as a singular one \n",
    "real_tagged = [] \n",
    "count = 0 \n",
    "while count < len(tagged): \n",
    "    if '\\'' in tagged[count][0]: \n",
    "        real_tuple = (tagged[count-1][0] + tagged[count][0], tagged[count-1][1])\n",
    "        # real_tagged[count-1][0] += tagged[count][0] \n",
    "        real_tagged[count-1] = real_tuple\n",
    "    else:\n",
    "        real_tagged.append(tagged[count])\n",
    "        # print(f'real_tagged: {real_tagged}')\n",
    "    count += 1\n",
    "print(real_tagged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40acbc06-0c8c-4ee4-bc39-987de44ee759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'I', \"didn't\", 'A-list', 'hit', 'his', '[mask]', 'I', 'was', 'at', 'the', 'store', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "list_txt = txt.split(\" \")\n",
    "print(list_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef0a62fa-acd1-438d-86c4-b44e899dbd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'pronoun', 'verb', 'noun', 'verb', 'pronoun', None, 'pronoun', 'verb', None, 'verb', 'noun', None]\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of {noun, adj, verb, adverb, prounoun} \n",
    "#map every part of speech to one of the above \n",
    "pos_vals = {\"D\", \"J\", \"N\", \"P\", \"V\", \"R\"}\n",
    "map_to_pos = {\"D\":\"verb\", \"J\":\"adj\", \"N\":\"noun\", \"P\":\"pronoun\", \"V\":\"verb\", \"R\": \"adverb\"}\n",
    "\n",
    "word_pos = [] \n",
    "indx = 0 \n",
    "for word, pos in real_tagged: \n",
    "    if not(word == \"[\" or word == \"mask\" or word == \"]\" or word == \"CLS\" or word == \"SEP\" or pos[0] not in pos_vals):\n",
    "        #for debugging\n",
    "        # word_pos.append((word, map_to_pos[pos[0]])) #change this back to just a value of map_to_pos(pos[0]) after debugging\n",
    "        word_pos.append(map_to_pos[pos[0]])\n",
    "    else:\n",
    "        if word == \"]\": \n",
    "            word_pos.pop(-1)\n",
    "            word_pos.pop(-1)\n",
    "        word_pos.append(None) \n",
    "    indx +=  1\n",
    "print(word_pos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65c5fcfa-0a0c-459d-b702-516b20f28954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allowed relationships \n",
    "\n",
    "relationships = {\"noun\": {\"verb\", \"adj\", \"pronoun\"},  \"verb\":{\"noun\", \"adverb\"}, \"adj\":{\"noun\", \"pronoun\"}, \"pronoun\":{\"verb\", \"adj\", \"adverb\", \"noun\"}, \"adverb\": {\"verb\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6311a4-a218-4a2f-bf89-ef524b7c3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6531f67f-d6e9-46b5-b2f4-e8b5d6357420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I didn't A-list hit his [mask] I was at the store [SEP]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mask = np.ones((len(list_txt), len(list_txt))) #including the [CLS] and [SEP] tags \n",
    "#relationships that we want noun->adj, adj->noun, verb->noun, noun->verb, adverb->verb, verb->adverb, pronouns->nouns, nouns->pronouns\n",
    "mask_indx = 6 #adjusted for [CLS] and [SEP] tokens \n",
    "\n",
    "for row in range(mask.shape[0]):\n",
    "    row_word_pos = word_pos[row] \n",
    "    if row_word_pos: #the [CLS], [SEP], and [MASK] tokens will be allowed to be influenced by every word \n",
    "        allowed_pos = relationships[row_word_pos] \n",
    "        for col in range(mask.shape[1]): \n",
    "            #for now, don't worry about the [mask] token not being allowed to influence the words \n",
    "            if word_pos[col] not in allowed_pos: \n",
    "                mask[row,col] = 0\n",
    "            # else:\n",
    "            #     mask[row, col] = 0 \n",
    "np.fill_diagonal(mask, 1) #allow words to attend to themselves\n",
    "\n",
    "print(txt)\n",
    "print(mask)    \n",
    "\n",
    "#the below is wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db16f711-5d21-49c7-b752-f66c7d96927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently this mask does not allow the [mask] token to attend to other words  \n",
    "def generate_mask(txt, debug=False):\n",
    "    txt = txt.replace(\"(\", \"\")\n",
    "    txt = txt.replace(\")\", \"\")\n",
    "    # txt = re.sub(' +', ' ', txt)\n",
    "    tokenized = sent_tokenize(txt)\n",
    "    for i in tokenized: \n",
    "        wordsList = nltk.word_tokenize(i) \n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "    real_tagged = [] \n",
    "    count = 0 \n",
    "    while count < len(tagged): \n",
    "        if '\\'' in tagged[count][0]: \n",
    "            real_tuple = (tagged[count-1][0] + tagged[count][0], tagged[count-1][1])\n",
    "            # real_tagged[count-1][0] += tagged[count][0] \n",
    "            # print(f'length: {len(real_tagged)}')\n",
    "            # print(f'count: {count}, {real_tagged}')\n",
    "            real_tagged[-1] = real_tuple\n",
    "        else:\n",
    "            real_tagged.append(tagged[count])\n",
    "            # print(f'real_tagged: {real_tagged}')\n",
    "        count += 1\n",
    "    if debug:\n",
    "        print(f'Tags: {real_tagged}')\n",
    "    list_txt = txt.split(\" \")\n",
    "\n",
    "    pos_vals = {\"D\", \"J\", \"N\", \"P\", \"V\", \"R\"}\n",
    "    map_to_pos = {\"D\":\"verb\", \"J\":\"adj\", \"N\":\"noun\", \"P\":\"pronoun\", \"V\":\"verb\", \"R\": \"adverb\"}\n",
    "    \n",
    "    word_pos = [] \n",
    "    indx = 0 \n",
    "    for word, pos in real_tagged: \n",
    "        if not(word == \"[\" or word == \"mask\" or word == \"]\" or word == \"CLS\" or word == \"SEP\" or pos[0] not in pos_vals):\n",
    "            #for debugging\n",
    "            # word_pos.append((word, map_to_pos[pos[0]])) #change this back to just a value of map_to_pos(pos[0]) after debugging\n",
    "            word_pos.append(map_to_pos[pos[0]])\n",
    "        else:\n",
    "            if word == \"]\": \n",
    "                word_pos.pop(-1)\n",
    "                word_pos.pop(-1)\n",
    "            word_pos.append(None) \n",
    "        indx +=  1\n",
    "\n",
    "    print(f'length of word pos (After removing extra spaces): {len(word_pos)}') \n",
    "    print(f'length of list_txt: {len(list_txt)}')\n",
    "    relationships = {\"noun\": {\"verb\", \"adj\", \"pronoun\"},  \"verb\":{\"noun\", \"adverb\", \"pronoun\"}, \"adj\":{\"noun\", \"pronoun\"}, \"pronoun\":{\"verb\", \"adj\", \"adverb\", \"noun\"}, \"adverb\": {\"verb\"}}\n",
    "\n",
    "    mask = np.ones((len(list_txt), len(list_txt))) #including the [CLS] and [SEP] tags \n",
    "    #relationships that we want noun->adj, adj->noun, verb->noun, noun->verb, adverb->verb, verb->adverb, pronouns->nouns, nouns->pronouns\n",
    "    mask_indx = 6 #adjusted for [CLS] and [SEP] tokens \n",
    "    \n",
    "    for row in range(mask.shape[0]):\n",
    "        row_word_pos = word_pos[row] \n",
    "        if row_word_pos: #the [CLS], [SEP], and [MASK] tokens will be allowed to be influenced by every word \n",
    "            allowed_pos = relationships[row_word_pos] \n",
    "            for col in range(mask.shape[1]): \n",
    "                #for now, don't worry about the [mask] token not being allowed to influence the words \n",
    "                if word_pos[col] not in allowed_pos: \n",
    "                    mask[row,col] = 0\n",
    "                # else:\n",
    "                #     mask[row, col] = 0 \n",
    "    np.fill_diagonal(mask, 1) #allow words to attend to themselves\n",
    "\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f40612ef-afc1-4c7c-a6b2-b77810cae0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'loony', '[mask]', 'have', 'ventured', 'at', 'least', 'twice', '[mask]', 'the', 'future', 'the', '[mask]', 'time', 'was', 'tung', 'the', 'brilliantly', 'funny', '[mask]', '[UNK]', 'the', 'latter', 'time', '[mask]', 'with', 'this', '\\x85', 'um', '\\x85', 'effort', '[UNK]', '[UNK]', 'isnt', 'without', 'merit', 'and', '[mask]', 'be', 'considered', 'a', 'good', 'product', 'style\"', 'it', 'not', 'that', 'it', 'isnt', '[mask]', '[mask]', 'warner', 'brothers', 'quality', 'wb', 'cartoons', 'whichever', 'noted', 'for', 'their', 'cheeky', 'humor', 'appealing', 'at', 'least', 'as', '[mask]', 'to', '[mask]', 'as', 'to', 'lavished', 'these', 'pedestrian', 'superhero', 'episodes', 'on', 'the', 'other', 'hand', '[mask]', 'fail', 'to', 'convince', 'adults', 'to', 'pass', 'them', 'up', 'the', 'premise', 'of', 'the', 'series', 'is', 'interpol', '[mask]', 'ordinary', 'individuals', '2', 'bunnies', 'a', '[UNK]', 'devil', 'a', 'duck', 'a', 'roadrunner', 'and', 'a', 'coyote', 'live', '[mask]', 'the', '[UNK]', 'of', '[UNK]', 'and', '[mask]', 'super', 'powers', 'when', 'a', '[mask]', 'strikes', 'the', 'planet', 'in', '[UNK]', 'whats', 'confusing', 'is', 'that', 'the', 'titles', 'section', 'features', 'these', 'individuals', 'with', 'a', '[UNK]', 'to', '[UNK]', 'from', 'the', '21st', 'century', 'cute', 'but', '[UNK]', 'stupid', 'in', 'each', 'episode', 'the', 'super', '[UNK]', '\\x96', 'amid', 'mildly', 'amusing', 'but', 'essentially', '[mask]', 'banter', '[mask]', 'fight', 'various', 'super', 'villains', 'for', 'the', '[mask]', 'part', 'these', 'are', 'types', 'that', 'appear', 'in', 'every', 'mediocre', 'superhero', 'adventure', 'series', 'and', 'even', 'some', 'of', 'the', 'better', 'ones', 'like', 'many', 'mediocre', 'superhero', 'series', 'this', 'one', 'takes', 'its', 'villains', 'far', 'too', 'seriously', 'for', 'the', 'living', 'and', 'of', 'course', 'these', 'guys', '[mask]', 'the', 'only', 'characters', 'that', 'laugh', '\\x96', 'the', 'usual', 'evil', 'laugh', 'of', 'course', 'why', '[mask]', 'it', 'that', 'villains', 'in', 'predictable', 'superhero', 'adventures', '[mask]', '\\x96', '[mask]', '\\x96', 'laugh', '[UNK]', 'at', 'every', '[mask]', 'animated', '[mask]', '[mask]', 'this', 'sort', 'seems', '[mask]', 'leave', 'laughter', 'exclusively', 'in', 'the', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "txt = \"[CLS] loony [mask] have ventured at least twice [mask] the future the [mask] time was tung the brilliantly funny [mask] [UNK] the latter time [mask] with this ",
    " um ",
    " effort [UNK] [UNK] isnt without merit and [mask] be considered a good product style\\\" it not that it isnt [mask] [mask] warner brothers quality wb cartoons whichever noted for their cheeky humor appealing at least as [mask] to [mask] as to lavished these pedestrian superhero episodes on the other hand [mask] fail to convince adults to pass them up the premise of the series is interpol [mask] ordinary individuals 2 bunnies a [UNK] devil a duck a roadrunner and a coyote live [mask] the [UNK] of [UNK] and [mask] super powers when a [mask] strikes the planet in [UNK] whats confusing is that the titles section features these individuals with a [UNK] to [UNK] from the 21st century cute but [UNK] stupid in each episode the super [UNK]  amid mildly amusing but essentially [mask] banter [mask] fight various super villains for the [mask] part these are types that appear in every mediocre superhero adventure series and even some of the better ones like many mediocre superhero series this one takes its villains far too seriously for the living and of course these guys [mask] the only characters that laugh  the usual evil laugh of course why [mask] it that villains in predictable superhero adventures [mask]  [mask]  laugh [UNK] at every [mask] animated [mask] [mask] this sort seems [mask] leave laughter exclusively in the [SEP]\"\n",
    "# mask = generate_mask(txt)\n",
    "# import re \n",
    "l = txt.split(\" \") \n",
    "print(l)\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(mask)\n",
    "# print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "104b0579-8e40-46e1-ad26-53e705bb5d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (1906263203.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    test_txt = txt.replace(\"\\\", \"\")\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "txt = \"[CLS] i have seen the film a few days back on a video tape and even though it was hard [mask] swallow it [mask] one take because of its length and story i liked it very much i was impressed first by the script and [mask] by the realization of [mask] script the film takes you on a ride but [mask] is not an easy joyful ride [mask] goes [mask] time and different [mask] regimes and shows the influence of them to ordinary peoples lives [mask] i loved was the inner logic the film followed striking which just like logic in life was rather illogical and confusing at times [mask] [mask] the end when [mask] thought about it all the events and twists made sense it makes no sense though to try to [UNK] the story as it spreads in more than 50 years of time i also liked very much [UNK] [UNK] character [UNK] and the way mcewan played alluded as some critics would saw with restless abandon what i didnt like [mask] [mask] was that rehabilitation think he later played characters that [mask] me of [UNK] in films like [UNK] romance\\\" [UNK] [mask] which i actually love and to some [mask] in \\\"the insulted and the [UNK] [UNK] i [UNK] [UNK] shows [mask] think what a great filmmaker andrei [UNK] was before [mask] went to hollywood and made forgettable films like [UNK] and [UNK] and less [mask] like [UNK] train\\\" i would prefer [UNK] [UNK] to them [SEP]\" \n",
    "test_txt = txt.replace(\"\\\", \"\")\n",
    "print(test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d44af6b3-8706-47d3-8e52-3a113c2fd057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', '\\x100', 'test']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"this is a \\x100 test\"\n",
    "txt.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "766bcff8-73d3-48d4-8576-230f910b36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "#Testing text vectorization on custom spaces (ex: \\x95) \n",
    "all_data = ['As predictable as a Hallmark card, but not without merit, The Rookie makes for a solid outing. Dennis Quaid, the most reasonable jock actor working today, is absolutely perfect as the science teacher turned baseball player Jimmy Morris. The film is never dumbed down for the children, as would be expected from a G rated film. As a sports film, The Rookie is one of the best I have seen since Any Given Sunday.',\n",
    " \"If The Man in the White Suit had been done in America, can't you see either Danny Kaye or Jerry Lewis trying on Alec Guinness's Sidney Stratton on for size?<br /><br />This is one of the best of Alec Guinness's films and certainly one of the best that Ealing Studios in the United Kingdom ever turned out. It's so perfectly fits within the time frame of the new Labour government and the society it was trying to build. It's amazing how in times of crisis capital and labor can agree.<br /><br />Alec Guinness this meek little schnook of a man is obsessed with the idea that he can invent clothing that will never need cleaning, that in fact repels all kinds of foreign matter the minute it touches the garment. <br /><br />He's a persistent cuss and he does succeed. Of course the implications haven't really been thought through about the kind of impact clothing like that will have on society. In the end everyone is chasing him down like they would a fugitive, almost like Peter Lorre from M or Orson Welles in The Stranger or even Robert Newton in Oliver Twist. <br /><br />It's the mark of a great comedy film that a potentially serious situation like that chase as described in some of the serious films I've mentioned can be played for laughs. Poor Guinness's suit is not only white and stain repellent, but it glows like a neon sign.<br /><br />Other than Guinness the best performances are from Cecil Parker as yet another pompous oaf, Joan Greenwood as his siren daughter and Ernest Thesiger the biggest clothing manufacturer in the UK> <br /><br />Come to think of it, did Paramount borrow that suit from Ealing and give it to John Travolta for Saturday Night Fever?\",\n",
    " \"I went to see this movie at our college theater thirty years ago because I liked Bruce Dern in Silent Running and Family Plot. To this day (sorry Jack Nicholson), it is still the dullest movie I've ever seen. It just went on and on with no discernible point and then - it just ended. The lights came up and I watched everyone looking around in confusion. Had the projectionist missed a reel? I've never had the urge to find out. All I remember about the movie is that it was a non-drama about some annoying college basketball players and their coach. The most enjoyable part of the movie was watching the totally mystified audience afterwords. Fortunately, this was just an exception for Jack, Bruce, and Karen Black.\"]\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data,\n",
    "    30000,\n",
    "    256,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "\n",
    "def decode(tokens):\n",
    "    return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "46783f51-d9d3-4f12-a78e-0be438538190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK] in the [UNK]'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize_layer([\"this is a move\"])[0]\n",
    "# test = \"[CLS] castle in the sky is undoubtedly a hayao miyazaki film after seeing it [mask] the first time im glad to say that [mask] doesnt [mask] on the contrary you get your times worth which means as to what [mask] films are [mask] that is nothing less than excellent produced early in his [mask] career castle in the sky anticipates many of the trade marks in his later movies [mask] strong but vote [mask] characters forced to grow [mask] due to external circumstances helped out by very interesting and [mask] times lovable supporting [mask] and of course the usual battle of nature versus civilization [mask] [mask] lots of it [mask] painted sceneries ",
    " but alas no pigs at least that [mask] noticed after [mask] i have only seen chronic once never the less miyazaki had already [mask] his theatrical debut [mask] [mask] earlier [mask] [UNK] which was a dress rehearsal for princess mononoke his magnum opus castle in the [mask] is set a bit a part from [mask] two with a soft action packed first 30 minutes resembling tactics tv series conan and his directed episodes of [UNK] holmes in here we are introduced to [mask] a girl who literally falls from the sky only to be found by pazu a [mask] boy working in a little [mask] mining town intrigued by her amnesia and suspecting a connection between her and the mysterious flying city of laputa pazu is set on helping her find out [mask] [mask] [mask] from whilst escaping the army and a gang of [SEP]\"\n",
    "test = \"castle in the sky\" \n",
    "decode(vectorize_layer([test])[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8abdf0f-aaad-40d1-834c-3992d9c3eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "test = '\\x96'\n",
    "print(ord(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d8df35ec-59d9-42b4-80be-cad2b28ba5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(vectorized_txt): \n",
    "    pos_vals = {\"D\", \"J\", \"N\", \"P\", \"V\", \"R\"}\n",
    "    map_to_pos = {\"D\":\"verb\", \"J\":\"adj\", \"N\":\"noun\", \"P\":\"pronoun\", \"V\":\"verb\", \"R\": \"adverb\"}\n",
    "    relationships = {\"noun\": {\"verb\", \"adj\", \"pronoun\"},  \"verb\":{\"noun\", \"adverb\", \"pronoun\"}, \"adj\":{\"noun\", \"pronoun\"}, \"pronoun\":{\"verb\", \"adj\", \"adverb\", \"noun\"}, \"adverb\": {\"verb\"}}\n",
    "\n",
    "    \n",
    "    decoded_txt = \"\" \n",
    "    word_count =  0 \n",
    "    #assuming the shape of vectorized_txt is (256,) \n",
    "    for i in range(len(vectorized_txt)):\n",
    "        vectorized_word = vectorized_txt[i] \n",
    "        if vectorized_word == 0:\n",
    "            break \n",
    "        else:\n",
    "            #doing this in case the last word is a specialized space token and it gets stripped at the end \n",
    "            if i != len(vectorized_txt)-1 and vectorized_txt[i+1] != 0 :\n",
    "                decoded_txt += decode([vectorized_word]) + \" \" \n",
    "            else:\n",
    "                decoded_txt += decode([vectorized_word]) \n",
    "            word_count += 1 \n",
    "    \n",
    "    tokenized = word_tokenize(decoded_txt) \n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    word_pos = []\n",
    "    for word, pos in tagged: \n",
    "        if word == ']':\n",
    "            word_pos.pop(-1)\n",
    "            word_pos.pop(-1)\n",
    "            word_pos.append(None)\n",
    "        if ord(word[0]) >= 127 and len(word) == 1 or pos[0] not in pos_vals: \n",
    "            word_pos.append(None)\n",
    "        else:\n",
    "            word_pos.append(map_to_pos[pos[0]]) \n",
    "\n",
    "    mask = np.ones((len(word_pos), len(word_pos)))\n",
    "\n",
    "    for row in range(mask.shape[0]):\n",
    "        row_word_pos = word_pos[row] \n",
    "        if row_word_pos: #the [CLS], [SEP], and [MASK] tokens will be allowed to be influenced by every word \n",
    "            allowed_pos = relationships[row_word_pos] \n",
    "            for col in range(mask.shape[1]): \n",
    "                #for now, don't worry about the [mask] token not being allowed to influence the words \n",
    "                if word_pos[col] not in allowed_pos: \n",
    "                    mask[row,col] = 0\n",
    "                # else:\n",
    "                #     mask[row, col] = 0 \n",
    "\n",
    "    np.fill_diagonal(mask, 1) \n",
    "\n",
    "    return mask \n",
    "                \n",
    "    # for vectorized_word in vectorized_txt: \n",
    "    #     if vectorized_word == 0:\n",
    "    #         break \n",
    "    #     else:\n",
    "    #         decoded_txt += decode(vectorized_word) + \" \"\n",
    "    #         word_count += 1 \n",
    "    # decoded_txt = decoded_txt.strip() \n",
    "\n",
    "    # tokenized = sent_tokenize(decoded_txt) \n",
    "\n",
    "\n",
    "    # mask = np.ones((len(tagged) + 2, len(tagged) + 2)) \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8faf30b8-bbcd-400b-8070-8bfd606b2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('\\x86', 'JJ'), ('test', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "temp = \"this is a \\x86 test\" \n",
    "tokenized = word_tokenize(temp) \n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0db6d0bf-3577-4200-8537-3354c49b6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "txt = \"\\x86\"\n",
    "print(txt)\n",
    "print(len(txt))\n",
    "print(ord('\\x80'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "24d78773-8e67-49bc-bfcc-63a74c41b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [    1,  8932,  3655, 29999, 21983,    30,   218,  1442, 29999,\n",
    "           2,   715,     2,    88,    58,    13,    15,     2,  2108,\n",
    "       29999,  4900,     1,     2,  1591,    58,    13,    15,    11,\n",
    "        5451,  7350,  5451,   759,     1,  8723, 29999,   200,  3626,\n",
    "           3,   227,    26,  1183, 29999,    49,  2138,    66,     9,\n",
    "          21,    12,     9,   202,    55, 29999,  2932,  1018,   484,\n",
    "       10145, 29999,    23,  3061,    16,    63, 13880,   454,  2411,\n",
    "          30, 29999,    14,    73,     6, 29999,    14,     6,   460,\n",
    "         129,  6850,  4437,   680,    20,     2,    76,   565,   560,\n",
    "        1864, 29999,  2356,  1508,     6,  1312,    90,    55,     2,\n",
    "         831,     5,     2,   209, 29999,    12,  1350,  1800,  3183,\n",
    "         283, 19353, 29999,     1, 29999,     4, 29999,     4, 29999,\n",
    "           3,     4,  9072,   433,    20,     2,     1,     5,     1,\n",
    "           3, 11201,  1576,  1764,    50,     4,  7267,  3620,     2,\n",
    "        1278,     8,     1,   691,  1417,     7,    12,     2,  2858,\n",
    "        2510,   889,   129,  3183,    15,     4,     1,     6,     1,\n",
    "          35,     2,  5900,  1105,  1020,    18,     1,   359,     8,\n",
    "         246,   389,     2, 29999,     1,   488,  8447, 29999,  1117,\n",
    "          18,  1904,  5351,  6941,   488,   547,   945,  1576, 29999,\n",
    "       29999,     2,    87,   170,   129,    23,  2123,    12,   952,\n",
    "           8, 29999,  1527, 29999,  1228,   209,     3,    54,    46,\n",
    "           5,     2,   123,   512,    38,   104, 29999,  4437,   209,\n",
    "          11,    28,   301, 29999,  1765,   226, 29999,   607, 29999,\n",
    "           2,  1986,     3,     5,   258,   129,   407,    23,     2,\n",
    "          60, 29999,    12,   434,   488,     2,   648,   468,   434,\n",
    "           5,   258, 29999,     7,     9,    12,  1765, 29999,   706,\n",
    "       29999,  2454,   205,   488,   205,   488, 29999,     1,    30,\n",
    "         166,  1396,  1258,   814,     5, 29999,   421,   180, 29999,\n",
    "         549,  2109,  6974,     8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "29d01c21-4b2c-44b5-9ab5-473edf995d56",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8932",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mask\n",
      "Cell \u001b[0;32mIn[146], line 17\u001b[0m, in \u001b[0;36mgenerate_mask\u001b[0;34m(vectorized_txt)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#doing this in case the last word is a specialized space token and it gets stripped at the end \u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vectorized_txt)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m vectorized_txt[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[0;32m---> 17\u001b[0m         decoded_txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvectorized_word\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         decoded_txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decode([vectorized_word]) \n",
      "Cell \u001b[0;32mIn[88], line 55\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(tokens):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[43mid2token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[88], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(tokens):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mid2token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 8932"
     ]
    }
   ],
   "source": [
    "mask = generate_mask(test)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af2ec8-134e-44f5-a3e4-419dfbac6a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
